---
title: "Assignment IV"
author: "Erika Lin"
date: "February 27, 2019"
output:
  word_document: default
  html_document: default
---


## Question 1: Prediction using Logistic Regression
We are going to perform perdiction on a voting dataset (files->assignments->assignment_4). The dataset contains the  party affliation of 435 congressional members along with voting record on 16 issues that were put to vote in a single year. The party affliation is indicated as a binary variable as a 1 if the congress-person was a member of the 'A' party and 0 otherwise. The vote is indicated as 1 (if the member voted yes) and 0 (if ithe member voted no).

a) You will notice that the class-split is fairly even in the dataset.

0 : 168 members
1 : 267 members

Using caret, create a rough 80-20 split of the dataset into training and testing. In other words, 80% of the data should comprise the training dataset and 20% of the data should comprise the testing dataset. Ensure that the class membership is even (in other words, the proportion of 1 and 0 in both training and testing should be the approximately the same)


NOTE: Set the seed to 476

```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert your code below
rm(list=ls())
library(plyr)
library(dplyr)
library(lattice) 
library(ggplot2)
library(caret)

vote <- read.csv("data_votes.csv",header=TRUE) %>%
  mutate_if(is.numeric, as.factor)

summary(vote)

set.seed(476) #set seed to 476

#Create train dataset 80% split, create test dataset 20% split 
split <- createDataPartition(vote$party_A, p=0.8, list=FALSE, times = 1)
train <- vote[split,]
test <- vote[-split,]
head(train)
head(test)

summary(as.factor(train$party_A))
summary(as.factor(test$party_A))

```

b) Perform a logistic regression (using glm) on the training dataset and perform a prediction on the test dataset. 

```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert your code below

#Logistic regression on the training dataset 
train.log <- glm(formula = party_A~., data=train, family ="binomial")
summary(train.log)

#Prediction on the test dataset
test$pred <- predict.glm(train.log, newdata=test, type = "response")

```


c) Fill the confusion matrix below using your predictions. Consider outcome 1 as being "positive" and a probability cutoff of 0.5 (i.e. if probability >= 0.5, assign the label 1). 

```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert your code below

test$pred <- ifelse(test$pred >=0.5,1,0)

class(test$party_A)
class(test$pred)

#TP
tp <- test %>%
  filter(party_A ==1 & pred==1)%>%
  nrow()

#FP
fp <- test %>%
  filter(party_A == 0 & pred==1)%>%
  nrow()

#FN
fn <- test %>%
  filter (party_A==1 & pred==0) %>%
  nrow()

#TN
tn <- test %>%
  filter(party_A ==0 & pred == 0)%>%
  nrow()

```

Table        |  Actual_positive | Actual_negative
-------------|------------------|----------------
Pred_positive|        50        |        0
Pred_negative|        3         |       33
  
  
d) Calculate the following: Sensitivity, Specificity, Positive Predictive Value, Negative Predictive Value, False Positive Rate, and Accuracy.

```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert code below
#Sensitivity 
tp/(tp+fn)

#Specificity 
tn/(tn+fp)

#Positive Predictive Value
tp/(tp+fp)

#Negative Predictive Value
tn/(tn+fn)

#False Positive Rate (1-specificity)
1 - (tn/(tn+fp))

#Accuracy
(tn+tp)/(tp+tn+fp+fn)

```

e) Calculate AUC (with 95% CI) using predicted probabilities

```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert code below

library(pROC)
#Creat prediction
test$pred <- predict.glm(train.log, newdata=test, type = "response")

prediction <- roc(response = test$party_A, predictor = test$pred, direction = "<")

#Calculate AUC with 95% CI on test dataset
auc_calc_test <- auc(prediction)
cat("AUC:", auc_calc_test, "\n")

ci_auc_calc_test <- ci.auc(prediction)
cat("95% CI:", ci_auc_calc_test, "\n")


```

## Question 2: Cross-validation
Write a program that will perform 3-fold cross-validation (using the caret package) on the above train dataset. Calculate AUC (with 95% CI) on the test (train)? dataset. 

NOTE : Set your seed as 156

```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert code here
library(plyr)
library(dplyr)
library(lubridate)
library(lattice) 
library(ggplot2)
library(caret)
library(pROC)

data(train)
set.seed(156) #set seed to 156

#3-fold cross-validation 
#train_folds <- createFolds(train$party_A, k=3, list = TRUE)
#head(train_folds)

#Training using GLM (3-fold cross-validation)
training_params <- trainControl(method="cv", number=3)
train.glm <- train(as.factor(party_A)~.,data=train,method="glm", trControl = training_params)

#Prediction using caret
yhat_glm <- predict(train.glm, newdata=test, type="prob")

#Create a prediction object
prediction2 <- roc(predictor = yhat_glm[,2], response=test$party_A, direction = "<")

#Calculate AUC with 95% CI on train dataset
auc_calc_test2 <- auc(prediction2)
cat("AUC:", auc_calc_test2, "\n")

ci_auc_calc_test2 <- ci.auc(prediction2)
cat("95$ CI:", ci_auc_calc_test2, "\n")


```


## Question 3: Hierarchical clustering
We are going to use the USArrests dataset. Load this using the following command 
```{r, echo =TRUE, message = FALSE, warning=FALSE}
d.in <- data.frame(USArrests)
head(d.in)
```

(a) Perform hierarchical clustering using average linkage and Euclidean distance. Write the code and show the plots below.

Ans.
```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert code here

#Euclidean distance 
dist_in <- dist(d.in, method = "euclidean")

#Visualize matrix 
temp_matrix <- as.matrix(dist_in)
print(temp_matrix[1:6,1:6])

#Show plots for average linkage
h_in_avg <- hclust(dist_in, method="average")
plot(h_in_avg, xlab="states",main="Dendrogram of US States by Average Linkage", cex=0.6)


```

(b) Perform hierarchical clustering using complete linkage and Euclidean distance, after scaling the variables to have a standard deviation of one. Write the code and show the plots below. 
```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert code here


d.in <- as.data.frame(scale(d.in))

#Euclidean distance 
dist_in_scaled <- dist(d.in, method = "euclidean")

#Visualize matrix
temp_matrix <- as.matrix(dist_in_scaled)
print(temp_matrix[1:6,1:6])

#Show plots for complete linkage
h_in_complete <- hclust(dist_in_scaled, method="complete")
plot(h_in_complete, xlab="states", main = "Dendrogram of US States by Complete Linkage",cex=0.6)

```


## Question 4: K-means clustering
Download the dataset kmeans_data.csv (Files->Assignments->Assignment_4).  The dataset contains randomly generated 100 observations of 2 variables, viz., X1 and X2

(a) Plot X1 vs. X2 (i.e. X1 on the x-axis) as a scatter-plot. Write the code below.
```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert code

kmeans_data <- read.csv("kmeans_data.csv",header=TRUE)
plot(kmeans_data$X1, kmeans_data$X2, xlab= "X1", ylab= "X2",main="Scatter Plot of X1 vs. X2")

```


(b) Perform a k-means clustering with $K$ = 3. Overlap the cluster labels on the scatter plot.
```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert code

#Set seed
set.seed(156)

means_cluster <- kmeans(kmeans_data, centers = 3)
names(means_cluster)

means_cluster$cluster
means_cluster$centers
means_cluster$tot.withinss #219.2
means_cluster$betweenss #1771.6

plot(kmeans_data$X1, kmeans_data$X2, xlab = "X1", ylab= "X2", main = "Clustered (k=3) Scatter Plot of X1 vs. X2", col=means_cluster$cluster,pch=19,cex=0.5)
text(kmeans_data$X1, kmeans_data$X2, labels=means_cluster$cluster,pos=4)
points(means_cluster$centers,col=1:3,pch=3,cex=1,lwd=3)


```

(c) Perform a k-means clustering with $K$ = 4. Overlap the cluster labels on the scatter plot.
```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert code 

#Set seed
set.seed(156)

means_cluster_2 <- kmeans(kmeans_data, centers = 4)
names(means_cluster_2)

means_cluster_2$cluster
means_cluster_2$centers
means_cluster_2$tot.withinss #197.5
means_cluster_2$betweenss #1793.3

plot(kmeans_data$X1, kmeans_data$X2, xlab = "X1", ylab= "X2", main = "Clustered (k=4) Scatter Plot of X1 vs. X2", col=means_cluster_2$cluster,pch=19,cex=0.5)
text(kmeans_data$X1, kmeans_data$X2, labels=means_cluster_2$cluster,pos=4)
points(means_cluster_2$centers,col=1:3,pch=3,cex=1,lwd=3)

```

(d) Which is a better $K$?      
Ans.The better K has a minimal "tot.withinss" value and a maximum "betweenss" value. The "tot.withinss" value should be smaller to indicate little difference within a group. The "betweenss" should be larger to indicate greater difference between groups. Therefore, the K=4 cluster is better. 


## Question 5: Similarity Metrics
You are given the the following distance matrix that describes the euclidean distance between cities.

Table     | BOS | NY  | DC  | CHI
----------|-----|-----|-----|-----
BOS       |  0  | 206 | 429 | 963
NY        | 206 |  0  | 233 | 802
DC        | 429 | 233 |  0  | 671
CHI       | 963 | 802 | 671 |  0

You are asked to perform a hierarchical clustering using single linkage. 

The nearest pair of cities is BOS and NY, at distance 206. 

(a) Re-calculate the distance-matrix based on the merged group BOS/NY. 

Ans. 

Cluster 1 = (BOS/NY) so recalculate matrix against DC, CHI
1) min[dist(BOS/NY),DC]
= min[dist(BOS,DC),(NY,DC)]
= min[429,233]
= 233

2) min[dist(BOS/NY),CHI]
= min[dist(BOS,CHI),(NY,CHI)]
= min[963,802]
= 802

Table     | BOS,NY | DC  | CHI
----------|--------|-----|-----|
BOS,NY    |  0     | 233 | 802 | 
DC        | 233    |  0  | 671 | 
CHI       | 802    | 671 |  0  | 

                                                                   
(b) Perform hierarchical clustering manually on paper (not using R code) until you reach two clusters. Show step-wise distance matrix calculations.

Ans. 

Cluster 2: [(BOS/NY),DC] so recalculate matrix against CHI 
1) min[dist(BOS/NY),DC),CHI]
= min[dist(BOS/NY,CHI),(DC,CHI)]
= min[802,671]
= 671

Table     | BOS,NY,DC  | CHI
----------|------------|-----|
BOS,NY,DC |      0     | 671 | 
CHI       |     671    |  0  | 

Cluster 3 (by default): [(BOS/NY/DC),CHI] 
    



