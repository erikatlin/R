---
title: "Assignment V"
author: "Erika Lin"
date: "March 5, 2019"
output: html_document
---


## Introduction
We are going to use a simulated two-class data set with 200 observations for training and 100 observations for testing, which includes two features, and in which there is a visible but non-linear separation between the two classes. Use the code below for creating such a dataset.

```{r, echo=TRUE}
rm(list = ls())
library(nnet)
library(plyr)
library(dplyr)
library(pROC)
library(caret)


# set a seed
set.seed(1)

# ---- Create a training set ---- #
# create a matrix with 200 rows and two colums with values sampled from a normal distribution.
x <- matrix(rnorm(200*2), ncol = 2)
# Introduce some non-linearity where we move points around
x[1:100,] <- x[1:100,] + 2
x[101:150,] <- x[101:150,] -2
# assign class labels
y <- c(rep(1, 150), rep(0, 50))
# this forms a training set
d.train <- data.frame(x = x, y = as.factor(y))
names(d.train) <- c("X1", "X2", "Y")


# ---- Create a test set ---- #
# create a matrix with 100 rows and two colums with values sampled from a normal distribution.
x <- matrix(rnorm(100*2), ncol = 2)
# Introduce some non-linearity where we move points around
x[1:25,] <- x[1:25,] + 2 # moves points to the top-right of a 2D space
x[26:75,] <- x[26:75,] -2 # moves points to the bottom-left of a 2D space
# assign class labels
y <- c(rep(1, 75), rep(0, 25)) 
# this forms a testing set
d.test <- data.frame(x = x, y = as.factor(y))
names(d.test) <- c("X1", "X2", "Y")
```




## Question 1
Create a scatter-plot of all data points in the training set color-labeled by their class type. You will notice that one class is in the center of all points of the other class. In other words, the separation between the classes is a circle around the points with Y as -1. Repeat the same for the testing set. 

```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert your code below

library(ggplot2)

sp_train <- ggplot(d.train, aes(x=X1, y=X2)) +
  geom_point(aes(color=factor(Y))) +
  theme_classic() 
  plot(sp_train + ggtitle("Scatterplot of Training Data"))
  

  
sp_test <- ggplot(d.test, aes(x=X1, y=X2))+
  geom_point(aes(color=factor(Y))) +
  theme_classic()
  plot(sp_test + ggtitle("Scatterplot of Testing Data"))



```



## Question 2
Buid a neural network with a variable hidden layer network size from 2 to 50. Feel free to explore different decay rates using "expand.grid" as shown in class. Perform testing on d.test and report the final AUC with 95% CI. 

```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert your code below
d.train$Y <- ifelse(d.train$Y ==1, "Yes", "No") #assign training dataset column Y to Yes if = 1 else = No
d.test$Y<- ifelse(d.test$Y ==1, "Yes", "No") #assign test dataset columnn Y to Yes if = 1 else = No

#set up a neural network
fit_control <- trainControl(method="cv",
                            number=3,
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)

#set neural network parameters
nnet_params <- expand.grid(size=seq(from=2,to=50,by=1), 
                           decay=5e-4)
head(nnet_params)

#training neural network
nn_train <- train(Y~.,
            data = d.train,
            method = "nnet",
            metric = "ROC",
            trControl = fit_control,
            tuneGrid = nnet_params,
            trace = FALSE)

print(nn_train)


#Prediction on the test dataset
prediction <- predict(nn_train, newdata=d.test, type = "prob")
d.test$Y_prediction <- prediction$Yes #prediction we are looking at is Yes column

#Calculate AUC and 95% CI on test dataset
calculate <- roc(response = d.test$Y, predictor = d.test$Y_prediction, direction = "<")
auc(calculate)
ci.auc(calculate)

```



## Question 3

1. Build a logistic regression prediction model using d.train. Test on d.test, and report your test AUC.


```{r, echo =TRUE, message = FALSE, warning=FALSE}
# Insert your code below
rm(list = ls())
library(nnet)
library(plyr)
library(dplyr)
library(pROC)
library(caret)


# set a seed
set.seed(1)

# ---- Create a training set ---- #
# create a matrix with 200 rows and two colums with values sampled from a normal distribution.
x <- matrix(rnorm(200*2), ncol = 2)
# Introduce some non-linearity where we move points around
x[1:100,] <- x[1:100,] + 2
x[101:150,] <- x[101:150,] -2
# assign class labels
y <- c(rep(1, 150), rep(0, 50))
# this forms a training set
d.train <- data.frame(x = x, y = as.factor(y))
names(d.train) <- c("X1", "X2", "Y")


# ---- Create a test set ---- #
# create a matrix with 100 rows and two colums with values sampled from a normal distribution.
x <- matrix(rnorm(100*2), ncol = 2)
# Introduce some non-linearity where we move points around
x[1:25,] <- x[1:25,] + 2 # moves points to the top-right of a 2D space
x[26:75,] <- x[26:75,] -2 # moves points to the bottom-left of a 2D space
# assign class labels
y <- c(rep(1, 75), rep(0, 25)) 
# this forms a testing set
d.test <- data.frame(x = x, y = as.factor(y))
names(d.test) <- c("X1", "X2", "Y")

#Logistic regression on the training dataset 

train.log <- glm(formula = Y~., data=d.train, family ="binomial")
summary(train.log)

#Prediction on the test dataset
d.test$Y_prediction <- predict.glm(train.log, newdata=d.test, type = "response") 
d.test$Y_prediction <- ifelse(d.test$Y_prediction >=0.5,1,0) #set this parameter to build a confusion matrix that is binary and numeric 

#Calculate AUC on test dataset
prediction <- roc(response = d.test$Y, predictor = d.test$Y_prediction, direction = "<")
auc(prediction)







```




2. Which of the two models leads to better performance? Explain in no more than 2 sentences why.        

Ans. The neural network model leads to better performance as we are comparing the test and train datasets against each other with a calculated AUC value that evaluates which model is best. The AUC value closest to 1 is desirable, which the neural network model has. 
